{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "![Exercise 1](Images/Exercise1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to find the stationary points by differentiating then finding x when the differential equals 0:\n",
    "$$\\frac{d(f(x))}{dx}=3x^2+12x-3=0\\Leftrightarrow x^2+4x-1=0$$\n",
    "$$x=\\frac{-4\\pm \\sqrt{16+4}}{2}\\Leftrightarrow x=\\frac{-4+\\sqrt{20}}{2}=-2+\\sqrt{5} \\vee x=\\frac{-4-\\sqrt{20}}{2}=-2-\\sqrt{5}$$\n",
    "\n",
    "Since it is a qubic equation, then it either has one local minimum and one local maximum, or it has a dual saddle point. In this case it has one local maximum at $x=-2-\\sqrt{5}$ and one local minimum at $x=-2+\\sqrt{5}$, because the $x^3$ is positive, meaning it goes upwards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "![Exercise 2](Images/Exercise2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equation is:\n",
    "$$\\mathbf{\\theta}_{i+1}=\\mathbf{\\theta}_i-\\gamma_i(\\nabla L(\\mathbf{\\theta}_i))^\\intercal=\\mathbf{\\theta}_i-\\gamma_i \\sum_{n=1}^{N}{(\\nabla L_n(\\mathbf{\\theta}_i))^\\intercal}$$\n",
    "This consideres the full batch of size N. When using a mini-batch size of one we have $N=1$:\n",
    "$$\\mathbf{\\theta}_{i+1}=\\mathbf{\\theta}_i-\\gamma_i(\\nabla L_1(\\mathbf{\\theta}_i))^\\intercal=\n",
    "  \\begin{bmatrix}\\theta_{i_1}-\\gamma_i \\frac{\\partial(L_1(\\mathbf{\\theta}_i))}{\\partial \\theta_{i_1}}\\\\\n",
    "                 \\theta_{i_2}-\\gamma_i \\frac{\\partial(L_1(\\mathbf{\\theta}_i))}{\\partial \\theta_{i_2}}\\\\\n",
    "                 \\vdots \\\\\n",
    "                 \\theta_{i_m}-\\gamma_i \\frac{\\partial(L_1(\\mathbf{\\theta}_i))}{\\partial \\theta_{i_m}}\\\\\\end{bmatrix}\n",
    "                 ,\\; \\mathbf{\\theta} \\in \\mathbb{R}^m$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "![Exercise 3](Images/Exercise3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub Exercise a\n",
    "True\n",
    "\n",
    "\n",
    "A set is convex if for all pairs $x,y$ in the set we have $\\theta x+(1-\\theta)y$, where $0\\leq\\theta\\leq 1$, is also in the set.\n",
    "\n",
    "Consider two convex sets $C_1$ and $C_2$. Their intersection $C_1 \\cap C_2$ includes all points that are in both sets. Consider x and y in this intersection, if the intersection is convex then we have:\n",
    "$$z=\\theta x+(1-\\theta)y\\in C_1\\cap C_2, \\: 0\\leq \\theta \\leq 1$$\n",
    "Now because $C_1$ is convex, we know that z exists in $C_1$. Now if $C_1 \\cap C_2$ was not convex, then this means that z is in $C_1$ but is not in $C_2$. However, since $C_2$ is convex then z has to be in $C_2$. This proves that if both $C_1$ and $C_2$ are convex, then their intersection is also a convex set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub Exercise b\n",
    "False\n",
    "\n",
    "Consider two convex sets $C_1$ and $C_2$ that do not intersect. Their union $C_1 \\cup C_2$ is not convex because for $x\\in C_1$ and $y\\in C_2$ there is for some $\\theta$ a point $\\theta x + (1-\\theta)y$ that is not in any of the two sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub Exercise c\n",
    "False\n",
    "\n",
    "Consider two convex sets $C_1$ and $C_2$ that look like circles, spheres, hyper-spheres you choose the dimensions. If they interesect, then the difference between the circle $C_1$ from the circle $C_2$ leads to crescent shape which is obviously not convex. Hence proved by common sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "![Exercise 4](Images/Exercise4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub Exercise a\n",
    "True\n",
    "\n",
    "Consider the sum of two convex functions $f(\\mathbf{x})$ and $g(\\mathbf{x})$ being $h(\\mathbf{x})=f(\\mathbf{x})+g(\\mathbf{x})$. if $h(\\mathbf{x})$ is convex then we have:\n",
    "$$h(\\mathbf{y})\\geq h(\\mathbf{x})+\\nabla_{\\mathbf{x}}h(\\mathbf{x})^\\intercal (\\mathbf{y-x})$$\n",
    "$$f(\\mathbf{y})+g(\\mathbf{y})\\geq f(\\mathbf{x})+g(\\mathbf{x})+\\nabla_{\\mathbf{x}}(f(\\mathbf{x})+g(\\mathbf{x}))^\\intercal (\\mathbf{y-x})$$\n",
    "$$f(\\mathbf{y})+g(\\mathbf{y})\\geq f(\\mathbf{x})+g(\\mathbf{x})+(\\nabla_{\\mathbf{x}}f(\\mathbf{x})+\\nabla_{\\mathbf{x}}g(\\mathbf{x}))^\\intercal (\\mathbf{y-x})$$\n",
    "$$f(\\mathbf{y})+g(\\mathbf{y})\\geq f(\\mathbf{x})+\\nabla_{\\mathbf{x}}f(\\mathbf{x})^\\intercal (\\mathbf{y-x})+g(\\mathbf{x})+\\nabla_{\\mathbf{x}}g(\\mathbf{x})^\\intercal (\\mathbf{y-x})$$\n",
    "\n",
    "Since $f(\\mathbf{y})\\geq f(\\mathbf{x})+\\nabla_{\\mathbf{x}}f(\\mathbf{x})^\\intercal (\\mathbf{y-x})$ and $g(\\mathbf{y})\\geq g(\\mathbf{x})+\\nabla_{\\mathbf{x}}g(\\mathbf{x})^\\intercal (\\mathbf{y-x})$ then the above inequality holds, and the sum of two convex functions is convex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub Exercise b\n",
    "True\n",
    "\n",
    "Through the same logic as above, the sum of two conex functions is convex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub Exercise c\n",
    "\n",
    "Consider the sum of two convex functions $f(\\mathbf{x})$ and $g(\\mathbf{x})$ being $h(\\mathbf{x})=f(\\mathbf{x})g(\\mathbf{x})$. if $h(\\mathbf{x})$ is convex then we have:\n",
    "$$h(\\mathbf{y})\\geq h(\\mathbf{x})+\\nabla_{\\mathbf{x}}h(\\mathbf{x})^\\intercal (\\mathbf{y-x})$$\n",
    "$$f(\\mathbf{y})g(\\mathbf{y})\\geq f(\\mathbf{x})g(\\mathbf{x})+\\nabla_{\\mathbf{x}}(f(\\mathbf{x})g(\\mathbf{x}))^\\intercal (\\mathbf{y-x})$$\n",
    "$$f(\\mathbf{y})g(\\mathbf{y})\\geq f(\\mathbf{x})g(\\mathbf{x})+(\\nabla_{\\mathbf{x}}(f(\\mathbf{x})g(\\mathbf{x})))^\\intercal (\\mathbf{y-x})$$\n",
    "$$f(\\mathbf{y})g(\\mathbf{y})\\geq f(\\mathbf{x})g(\\mathbf{x})+(f(\\mathbf{x})\\nabla_{\\mathbf{x}}g(\\mathbf{x})+\\nabla_{\\mathbf{x}}f(\\mathbf{x})g(\\mathbf{x}))^\\intercal (\\mathbf{y-x})$$\n",
    "$$f(\\mathbf{y})g(\\mathbf{y})\\geq f(\\mathbf{x})g(\\mathbf{x})+(f(\\mathbf{x})\\nabla_{\\mathbf{x}}g(\\mathbf{x}))^\\intercal (\\mathbf{y-x})+(\\nabla_{\\mathbf{x}}f(\\mathbf{x})g(\\mathbf{x}))^\\intercal (\\mathbf{y-x})$$\n",
    "$$f(\\mathbf{y})g(\\mathbf{y})\\geq f(\\mathbf{x})g(\\mathbf{x})+f(\\mathbf{x})\\nabla_{\\mathbf{x}}g(\\mathbf{x})^\\intercal (\\mathbf{y-x})+g(\\mathbf{x})\\nabla_{\\mathbf{x}}f(\\mathbf{x})^\\intercal (\\mathbf{y-x})$$\n",
    "$$f(\\mathbf{y})g(\\mathbf{y})\\geq f(\\mathbf{x})(g(\\mathbf{x})+\\nabla_{\\mathbf{x}}g(\\mathbf{x})^\\intercal (\\mathbf{y-x}))+g(\\mathbf{x})\\nabla_{\\mathbf{x}}f(\\mathbf{x})^\\intercal (\\mathbf{y-x})$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "False\n",
    "\n",
    "I think I found a counter example. Consider convex $f(x)$ and convex $g(x)=-1$. Not consider the product of these two convex functions $h(x)=f(x)g(x)=-f(x)$. Using this we show that if:\n",
    "$$f(y)\\geq f(x)+\\nabla_{x}f(x)^\\intercal (y-x)$$\n",
    "Then by multiplying both sides by $g(x)$ we get:\n",
    "$$-f(y)\\leq -f(x)-\\nabla_{x}f(x)^\\intercal (y-x)\\Leftrightarrow h(y)\\leq h(x)+\\nabla_{x}h(x)^\\intercal (y-x)$$\n",
    "Showing that the product of two convex functions is not always convex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True\n",
    "\n",
    "When taking the max of two convex functions we have to consider 2 cases. Case one is that both $f(y)$ and $f(x)$ are bigger than $g(x)$ and $g(y)$. The other case is that $f(y)$ is bigger than $g(y)$, but f(x) is smaller than g(x). In the first case:\n",
    "$$max(f(y),g(y))=f(y)\\geq f(x)+\\nabla_{x}f(x)^\\intercal (y-x)=max(f(x),g(x))+\\nabla_{x}max(f(x),g(x))^\\intercal (y-x)$$\n",
    "Is true because $f(x)$ is convex. For the second case:\n",
    "$$max(f(y),g(y))=g(y)\\geq f(x)\\geq f(x)+\\nabla_{x}f(x)^\\intercal (y-x)=max(f(x),g(x))+\\nabla_{x}max(f(x),g(x))^\\intercal (y-x)$$\n",
    "Is also true because $f(x)$ is convex. The same reasoning works for when both $g(x)$ and $g(y)$ are bigger than $f(x)$ and $f(y)$ and when $g(y)$ is bigger than $f(y)$ but $f(x)$ is bigger than $g(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5\n",
    "![Exercise 5](Images/Exercise5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimization problem can become:\n",
    "$$\\max_{\\mathbf{x}\\in\\mathbb{R}^2, \\xi\\in\\mathbb{R}}{\\mathbf{p}^\\intercal\\mathbf{x}+\\xi}\\min_{\\mathbf{x}\\in\\mathbb{R}^2, \\xi\\in\\mathbb{R}}{-\\mathbf{p}^\\intercal\\mathbf{x}-\\xi}=\n",
    "  \\min_{\\mathbf{z}\\in\\mathbb{R}^3}{-\\mathbf{q}^\\intercal\\mathbf{z}}$$\n",
    "Where\n",
    "$$\\mathbf{q}=\\begin{bmatrix}p_0\\\\ p_1\\\\ 1\\end{bmatrix},\\: \\mathbf{z}=\\begin{bmatrix}x_0\\\\ x_1\\\\ \\xi\\end{bmatrix}$$\n",
    "This is subject to the constraint:\n",
    "$$\\begin{bmatrix}1&0&0\\\\0&1&0\\\\0&0&-1\\end{bmatrix}\\mathbf{z}\\leq\\begin{bmatrix}0\\\\ 3\\\\ 0\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6\n",
    "![Exercise 6](Images/Exercise6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dual linear program using Lagrange duality is:\n",
    "$$\\max_{\\mathbf{\\lambda}\\in\\mathbb{R}^5}{-\\mathbf{b}^\\intercal\\mathbf{\\lambda}}=\\max_{\\mathbf{\\lambda}\\in\\mathbb{R}^5}{\\begin{bmatrix}33&8&5&-1&8\\end{bmatrix}\\mathbf{\\lambda}}$$\n",
    "Subject to:\n",
    "$$\\mathbf{c}+\\mathbf{A}^\\intercal\\mathbf{\\lambda}=-\\begin{bmatrix}5\\\\3\\end{bmatrix}+\\begin{bmatrix}2&2&-2&0&0\\\\2&-4&1&-1&1\\end{bmatrix}\\lambda\\leq 0$$\n",
    "$$-\\mathbf{c}-\\mathbf{A}^\\intercal\\mathbf{\\lambda}=\\begin{bmatrix}5\\\\3\\end{bmatrix}-\\begin{bmatrix}2&2&-2&0&0\\\\2&-4&1&-1&1\\end{bmatrix}\\lambda\\leq 0$$\n",
    "$$-\\mathbf{I}_5\\lambda\\leq 0$$\n",
    "Which condenses to:\n",
    "$$\\begin{bmatrix}2&2&-2&0&0\\\\2&-4&1&-1&1\\\\-2&-2&2&0&0\\\\-2&4&-1&1&-1\\\\-1&0&0&0&0\\\\0&-1&0&0&0\\\\0&0&-1&0&0\\\\0&0&0&-1&0\\\\0&0&0&0&-1\\end{bmatrix}\\lambda\\leq\\begin{bmatrix}5\\\\3\\\\-5\\\\-3\\\\0\\\\0\\\\0\\\\0\\\\0\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7\n",
    "![Exercise 7](Images/Exercise7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dual quadratic program using Lagrange duality is:\n",
    "$$\\max_{\\mathbf{\\lambda}\\in\\mathbb{R}^4}{-\\frac{1}{2}\\mathbf{(c+A^\\intercal\\lambda)^\\intercal Q^{-1}(c+A^\\intercal\\lambda)-\\lambda^\\intercal b}}=\n",
    "  \\max_{\\mathbf{\\lambda}\\in\\mathbb{R}^4}{-\\frac{1}{2}\\mathbf{(\\begin{bmatrix}5\\\\3\\end{bmatrix}+\\begin{bmatrix}1&-1&0&0\\\\0&0&1&-1\\end{bmatrix}\\lambda)^\\intercal \\begin{bmatrix}\\frac{4}{7}&-\\frac{1}{7}\\\\-\\frac{1}{7}&\\frac{2}{7}\\end{bmatrix}(\\begin{bmatrix}5\\\\3\\end{bmatrix}+\\begin{bmatrix}1&-1&0&0\\\\0&0&1&-1\\end{bmatrix}\\lambda)-\\lambda^\\intercal \\begin{bmatrix}1\\\\1\\\\1\\\\1\\end{bmatrix}}}$$\n",
    "Subject to:\n",
    "$$-\\mathbf{I}_4\\mathbf{\\lambda}\\leq\\mathbf{0}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 8\n",
    "![Exercise 8](Images/Exercise8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we write down the Lagrangian:\n",
    "$$\\mathfrak{L}(\\mathbf{w},\\lambda)=\\frac{1}{2}\\mathbf{w^\\intercal w}+\\lambda(1-\\mathbf{w^\\intercal x})$$\n",
    "Now the function to be maximized in the dual problem is:\n",
    "$$\\mathfrak{D}(\\lambda)=\\min_{\\mathbf{w}\\in\\mathbb{R}^D}{\\mathfrak{L}(\\mathbf{w},\\lambda)}$$\n",
    "Taking the Jacobian (Note that the transpose is removed for convenience) of the Lagrangian and setting it equal to 0:\n",
    "$$\\mathbf{w}-\\lambda\\mathbf{x}=0\\Leftrightarrow\\mathbf{w}=\\lambda\\mathbf{x}$$\n",
    "Setting this into the Lagrangian we get:\n",
    "$$\\min_{\\mathbf{w}\\in\\mathbb{R}^D}{\\mathfrak{L}(\\mathbf{w},\\lambda)}=\\frac{\\lambda^2}{2}\\mathbf{x^\\intercal x}-\\lambda^2\\mathbf{x^\\intercal x}+\\lambda=\\lambda-\\frac{\\mathbf{x^\\intercal x}}{2}\\lambda^2=\\mathfrak{D}(\\lambda)$$\n",
    "Leading to the dual optimization problem:\n",
    "$$\\max_{\\lambda\\in\\mathbb{R}}{\\lambda-\\frac{\\mathbf{x^\\intercal x}}{2}\\lambda^2}$$\n",
    "Subject to:\n",
    "$$-\\lambda\\leq 1$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 9\n",
    "![Exercise 9](Images/Exercise9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The convex conjugate is:\n",
    "$$f^*(\\mathbf{s})=\\sup_{\\mathbf{x}\\in\\mathbb{R}^D}(\\mathbf{s^\\intercal x}-f(\\mathbf{x}))=\\sup_{\\mathbf{x}\\in\\mathbb{R}^D}\\left(\\mathbf{s^\\intercal x}-\\sum_{d=1}^{D}{x_dlog(x_d)}\\right)$$\n",
    "I do not know the difference between supremum and maximum in these cases, so I will take the gradient and find the maximum:\n",
    "$$\\mathbf{s}-\\begin{bmatrix}1+log(x_1)\\\\1+log(x_2)\\\\ \\vdots\\\\1+log(x_D)\\end{bmatrix}=0\\Leftrightarrow\\mathbf{x}=\\begin{bmatrix}e^{s_1-1}\\\\ e^{s_2-1}\\\\ \\vdots\\\\ e^{s_D-1}\\end{bmatrix}$$\n",
    "Putting this instead of $\\mathbf{x}$ we get:\n",
    "$$f^*(\\mathbf{s})=\\sum_{d=1}^{D}{s_d\\cdot e^{s_d-1}}-\\sum_{d=1}^{D}{e^{s_d-1}(s_d-1)}=\\sum_{d=1}^{D}{s_d\\cdot e^{s_d-1}-s_d\\cdot e^{s_d-1}+e^{s_d-1}}=\\sum_{d=1}^{D}{e^{s_d-1}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 10\n",
    "![Exercise 10](Images/Exercise10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The convex conjugate is:\n",
    "$$f^*(\\mathbf{s})=\\sup_{\\mathbf{x}\\in\\mathbb{R}^D}(\\mathbf{s^\\intercal x}-f(\\mathbf{x}))=\\sup_{\\mathbf{x}\\in\\mathbb{R}^D}(\\mathbf{s^\\intercal x}-\\frac{1}{2}\\mathbf{x^\\intercal Ax-b^\\intercal x}-c)$$\n",
    "Since $\\mathbf{A}$ is positive definite, then it must be symmetric, and the jacobian of $\\mathbf{x^\\intercal Ax}$ is $\\mathbf{2Ax}$. This means that differentiating $f(\\mathbf{x})$ with respect to $\\mathbf{x}$ we get and setting it to 0:\n",
    "$$(s-\\mathbf{Ax}-\\mathbf{b})^\\intercal=0\\Leftrightarrow \\mathbf{x}=\\mathbf{A^{-1}(s-b)}$$\n",
    "Replacing x with that we get:\n",
    "$$f^*(\\mathbf{s})=\\mathbf{s^\\intercal A^{-1}(s-b)}-\\frac{1}{2}\\mathbf{(A^{-1}(s-b))^\\intercal AA^{-1}(s-b)-b^\\intercal A^{-1}(s-b)}-c$$\n",
    "$$f^*(\\mathbf{s})=\\mathbf{s^\\intercal A^{-1}(s-b)-b^\\intercal A^{-1}(s-b)}-\\frac{1}{2}\\mathbf{(s-b)^\\intercal A^{-1}(s-b)}-c$$\n",
    "$$f^*(\\mathbf{s})=\\mathbf{(s-b)^\\intercal A^{-1}(s-b)}-\\frac{1}{2}\\mathbf{(s-b)^\\intercal A^{-1}(s-b)}-c=\\frac{1}{2}\\mathbf{(s-b)^\\intercal A^{-1}(s-b)}-c$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 11\n",
    "![Exercise 11](Images/Exercise11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The convex conjugate of the hinge loss is:\n",
    "$$L^*(\\beta)=\\sup_{\\alpha}(\\beta\\alpha-L(\\alpha))=\\sup_{\\alpha}(\\beta\\alpha-\\max_{\\alpha}(0,1-\\alpha))=\\sup_{\\alpha}{\\begin{cases}\\beta\\alpha&\\text{if } \\alpha\\geq 1\\\\ \\beta\\alpha-1+\\alpha&\\text{if } \\alpha< 1\\end{cases}}=\\sup_{\\alpha}{\\begin{cases}\\beta\\alpha&\\text{if } \\alpha\\geq 1\\\\ (\\beta+1)\\alpha-1&\\text{if } \\alpha< 1\\end{cases}}$$\n",
    "When $\\beta>0$ the supremum is at $+\\infty$ due to $\\beta\\alpha$, when $\\beta<-1$ the supremum is at $+\\infty$ due to $(\\beta+1)\\alpha-1$. Lastly when $\\beta$ is between these two values, then the supremum is $\\beta$ because the funciton increases until it hits $\\beta$ then starts decreasing again:\n",
    "$$L^*(\\beta)=\\begin{cases}\\beta&\\text{if } -1\\leq\\beta\\leq 0\\\\+\\infty&\\text{otherwise}\\end{cases}$$\n",
    "After adding the $l_2$ proximal term we have:\n",
    "$$\\begin{cases}\\beta+\\frac{\\gamma}{2}\\beta^2&\\text{if } -1\\leq\\beta\\leq 0\\\\+\\infty&\\text{otherwise}\\end{cases}$$\n",
    "The dual of conjugate of that is:\n",
    "$$L^*(c)=\\sup_{\\beta}{\\begin{cases}c\\beta-\\beta-\\frac{\\gamma}{2}\\beta^2&\\text{if } -1\\leq\\beta\\leq 0\\\\-\\infty&\\text{otherwise}\\end{cases}}=\\sup_{\\beta}{\\begin{cases}(c-1)\\beta-\\frac{\\gamma}{2}\\beta^2&\\text{if } -1\\leq\\beta\\leq 0\\\\-\\infty&\\text{otherwise}\\end{cases}}=\\sup_{\\beta}{\\begin{cases}((c-1)-\\frac{\\gamma}{2}\\beta)\\beta&\\text{if } -1\\leq\\beta\\leq 0\\\\-\\infty&\\text{otherwise}\\end{cases}}$$\n",
    "The maximum of the quadratic is found at the mid point of the two roots which are $\\beta=0$ and $\\beta=\\frac{2(c-1)}{\\gamma}$, meaning that supremum is when $\\beta=\\frac{c-1}{\\gamma}$ leading to:\n",
    "$$L^*(c)=\\frac{(c-1)^2}{\\gamma}-\\frac{\\gamma(c-1)^2}{2}$$\n",
    "Given $(1-\\gamma)\\leq c\\leq 1$, so that it stays withing $-1\\leq\\beta\\leq 0$. In case $c<(1-\\gamma)$, then the edge of the quadratic is the supremum, which is found when $\\beta=(1-\\gamma)$:\n",
    "$$L^*(c)=(c-1)(1-\\gamma)-\\frac{\\gamma}{2}(1-\\gamma)^2$$\n",
    "Lastly in the case $c>1$, then the supremum is 0. This leads to:\n",
    "$$L^*(c)=\\begin{cases}(c-1)(1-\\gamma)-\\frac{\\gamma}{2}(1-\\gamma)^2&\\text{if }c<(1-\\gamma)\\\\\\frac{(c-1)^2}{\\gamma}-\\frac{\\gamma(c-1)^2}{2}&\\text{if }(1-\\gamma)\\leq c\\leq 1\\\\0&\\text{if }c> 1\\end{cases}$$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
